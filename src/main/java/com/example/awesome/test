case class Memoize[A1, A2, B](f: (A1, A2) => B) extends ((A1, A2) => B) {
  val cache = scala.collection.mutable.Map.empty[(A1, A2), B]

  def apply(x: A1, y: A2) = cache.getOrElseUpdate((x, y), f(x, y))
}

object LCSM {
  lazy val lcsM: Memoize[Seq[String], Seq[String], Seq[String]] = Memoize {
    case (_, Nil) => Nil
    case (Nil, _) => Nil
    case (x :: xs, y :: ys) if x == y => x +: lcsM(xs, ys)
    case (x :: xs, y :: ys) => {
      (lcsM(x :: xs, ys), lcsM(xs, y :: ys)) match {
        case (a, b) if a.length > b.length => a
        case (_, d) => d
      }
    }
  }

  def isSubSequence(a: List[String], b: List[String]): Boolean = {
    a.size == lcsM(a, b).size
  }

}


import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD

/**
  * Created by AXH2716 on 1/17/2017.
  */
class CustomAssociationRules(rdd: RDD[(Long, Set[String])], sc: SparkContext) {

  val transactionalData: RDD[(Long, Set[String])] = rdd
  val n = rdd.count()
  val products = distinctProducts()

  /*returns support for a single itemset*/
    def support(products: Set[String]): Double = {
      val prodTransactionCounts = transactionalData.map(_._2).filter(x => products.subsetOf(x)).count().toDouble
      prodTransactionCounts / n
    }

    /*returns support for a single item*/
    def support(product: String): Double = {
      val prodTransactionCounts = transactionalData.map(_._2).filter(x => Set(product).subsetOf(x)).count().toDouble
      prodTransactionCounts / n
  }

  /*returns the lift measure for an association rule*/
  def lift(prodA: Set[String], prodB: Set[String]): Double = {
    confidence(prodA, prodB) / support(prodB)
  }

  /*returns the confidence measure for an association rule*/
  def confidence(prodA: Set[String], prodB: Set[String]): Double = {
    support(prodA, prodB) / support(prodA)
  }

  /*returns support for an association rule based on two itemsets*/
  def support(prodA: Set[String], prodB: Set[String]): Double = {
    val prodTransactionCounts = transactionalData.map(_._2).filter(x => prodA.subsetOf(x) && prodB.subsetOf(x)).count().toDouble
    prodTransactionCounts / n
  }

  /*distinct products in the transactional data*/
  def distinctProducts(): Set[String] = {
    transactionalData.flatMap(_._2).distinct().collect().toSet.filter(_.nonEmpty)
  }

  /*calculates candidate-1 itemsets*/
  def candidateOneItemSets(minSupport: Double): Set[String] = {
    products.filter(x => support(x) >= minSupport)
  }

  /*generates K-frequent itemsets*/
  def generateKFrequentItemSets(k: Int, minSupport: Double): Set[Set[String]] = {
    products.subsets(k).filter(x => support(x) >= minSupport).toSet
  }

  /*prunes itemset combinations and selects those with support>minsuppor*/
  def prune(k: Int, minSupport: Double): RDD[Set[String]] = {
    sc.parallelize({
      1 to k
    }.flatMap(x => generateKFrequentItemSets(x, minSupport)))
  }

  /*runs the association rules over combinations of the pruned itemset*/
  def run(k: Int, minSupport: Double): RDD[(Set[String], Set[String], Double, Double, Double)] = {
    val frequentItemSets = prune(k, minSupport).collect()

    sc.parallelize(frequentItemSets.flatMap(f = lhs => frequentItemSets.map { rhs =>
      if (!lhs.equals(rhs))
        (lhs, rhs, support(lhs, rhs), confidence(lhs, rhs), lift(lhs, rhs))
      else
        null
    }).filter(x => x != null))

  }
  
  def haversineDistance(geocode1: Geocode, geocode2: Geocode): Double = {
    val deltaLat = math.toRadians(geocode2.longitude - geocode1.longitude)
    val deltaLong = math.toRadians(geocode2.latitude - geocode1.latitude)
    val a = math.pow(math.sin(deltaLat / 2), 2)
    + math.cos(math.toRadians(geocode1.longitude)) * math.cos(math.toRadians(geocode1.longitude)) * math.pow(math.sin(deltaLong / 2), 2)
    val greatCircleDistance = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    3958.761 * greatCircleDistance
  }
}
